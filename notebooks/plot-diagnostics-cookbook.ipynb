{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Diagnostic Plots for NA-CORDEX Zarr Stores\n",
    "\n",
    "This cookbook provides useful methods for summarizing data values in large climate datasets.   It clearly shows where data have extreme values and where data are missing; this can be useful for validating that the data were gathered and stored correctly.   While this cookbook is specifically designed to examine any portion of the NA-CORDEX dataset using an intake catalog, the code can be adapted straightforwardly to other datasets that can be loaded via `xarray`.\n",
    "\n",
    "The main python packages used are `xarray`, `intake-esm`, `dask`, and `matplotlib`.\n",
    "\n",
    "* [NA-CORDEX on AWS Documentation](https://na-cordex.org/na-cordex-on-aws.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import intake\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore dask-numpy warnings about finding missing values in the data\n",
    "%env PYTHONWARNINGS=ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select User Options\n",
    "\n",
    "Users may choose between:\n",
    "\n",
    "* Cloud storage provider (Amazon AWS or NCAR), \n",
    "* Whether to truncate the data if running on a resource-limited computer,\n",
    "* Whether to obtain a Dask cluster from a PBS Scheduler or via the Dask Gateway package.\n",
    "\n",
    "**Note**: Using the NCAR cloud storage system requires a login account on an NCAR HPC computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Choose Cloud Storage (Amazon AWS or NCAR Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If True,  use NCAR Cloud Storage.   Requires an NCAR user account.\n",
    "# If False, use AWS  Cloud Storage.\n",
    "\n",
    "USE_NCAR_CLOUD = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Choose whether to truncate data for resource-limited computers\n",
    "\n",
    "When running this notebook where compute resources are limited, or data transfer rates are slow, set `TRUNCATE_DATA` to True.   This will limit time ranges to 3 years from the start of a chosen NA-CORDEX scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRUNCATE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Choose whether to use a PBS Scheduler\n",
    "\n",
    "If running on a HPC computer with a PBS Scheduler, set to True.  Otherwise, set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_PBS_SCHEDULER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Choose whether to use Dask Gateway\n",
    "If running on Jupyter server with Dask Gateway configured, set to True.  Otherwise, set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_DASK_GATEWAY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Connect to Dask Distributed Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gateway_cluster():\n",
    "    \"\"\" Create cluster through dask_gateway\n",
    "    \"\"\"\n",
    "    from dask_gateway import Gateway\n",
    "\n",
    "    gateway = Gateway()\n",
    "    cluster = gateway.new_cluster()\n",
    "    cluster.adapt(minimum=2, maximum=20)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pbs_cluster():\n",
    "    \"\"\" Create cluster through dask_jobqueue\n",
    "    \"\"\"\n",
    "    from dask_jobqueue import PBSCluster\n",
    "    \n",
    "    if TRUNCATE_DATA:\n",
    "        num_jobs = 4 \n",
    "        walltime = '0:10:00' \n",
    "        memory = '2GB' \n",
    "    else:\n",
    "        num_jobs = 20\n",
    "        walltime = '0:20:00'\n",
    "        memory = '10GB' \n",
    "\n",
    "    cluster = PBSCluster(cores=1, processes=1, walltime=walltime, memory=memory, queue='casper', \n",
    "                         resource_spec=f\"select=1:ncpus=1:mem={memory}\",)\n",
    "    cluster.scale(jobs=num_jobs)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_cluster():\n",
    "    \"\"\" Create cluster using the Jupyter server's resources\n",
    "    \"\"\"\n",
    "    from distributed import LocalCluster\n",
    "    cluster = LocalCluster()    \n",
    "\n",
    "    if TRUNCATE_DATA:\n",
    "        cluster.scale(4)\n",
    "    else:\n",
    "        cluster.scale(20)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain dask cluster in one of three ways\n",
    "\n",
    "if USE_PBS_SCHEDULER:\n",
    "    cluster = get_pbs_cluster()\n",
    "elif USE_DASK_GATEWAY:\n",
    "    cluster = get_gateway_cluster()\n",
    "else:\n",
    "    cluster = get_local_cluster()\n",
    "\n",
    "# Connect to cluster\n",
    "from distributed import Client\n",
    "client = Client(cluster)\n",
    "\n",
    "# Display cluster dashboard URL\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ Link to Dask dashboard will appear above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Find and Obtain Data Using an Intake Catalog\n",
    "\n",
    "- [Intake-esm Documentation](https://intake-esm.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Intake Catalog URL and Storage Access Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NCAR_CLOUD:\n",
    "    catalog_url = \"https://stratus.ucar.edu/ncar-na-cordex/catalogs/aws-na-cordex.json\"\n",
    "    storage_options={\"anon\": True, 'client_kwargs':{\"endpoint_url\":\"https://stratus.ucar.edu/\"}}\n",
    "                     \n",
    "else:\n",
    "    catalog_url = \"https://ncar-na-cordex.s3-us-west-2.amazonaws.com/catalogs/aws-na-cordex.json\"\n",
    "    storage_options={\"anon\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open catalog and produce a content summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have the catalog interpret the \"na-cordex-models\" column as a list of values, as opposed to single values.\n",
    "col = intake.open_esm_datastore(catalog_url, read_csv_kwargs={\"converters\": {\"na-cordex-models\": ast.literal_eval}},)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few lines of the catalog\n",
    "col.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a catalog content summary.\n",
    "import pprint\n",
    "\n",
    "uniques = col.unique()\n",
    "\n",
    "columns = [\"variable\", \"scenario\", \"grid\", \"na-cordex-models\", \"bias_correction\"]\n",
    "for column in columns:\n",
    "    print(f'{column}: {uniques[column]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into xarray using the catalog\n",
    "\n",
    "Choose any combination of variable, grid, scenario, and bias correction listed in the catalog. \n",
    "\n",
    "Below we choose the variable 'tmax' (Maximum Daily Surface Temperature) as a default for its ease of interpretation.   For this example, we also choose a lower-resolution grid and the scenario with the shortest time range (15 years) to reduce the default data processing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var = 'tmax'\n",
    "\n",
    "col_subset = col.search(\n",
    "    variable=data_var,\n",
    "    grid=\"NAM-44i\",\n",
    "    scenario=\"eval\",\n",
    "    bias_correction=\"raw\",\n",
    ")\n",
    "\n",
    "col_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our query matches at least one entry in the catalog.\n",
    "\n",
    "col_subset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalog entries for subset into a dictionary of xarray datasets, and open the first one.\n",
    "\n",
    "dsets = col_subset.to_dataset_dict(\n",
    "    xarray_open_kwargs={\"consolidated\": True}, storage_options=storage_options\n",
    ")\n",
    "print(f\"\\nDataset dictionary keys:\\n {dsets.keys()}\")\n",
    "\n",
    "# Load the first dataset and display a summary.\n",
    "dataset_key = list(dsets.keys())[0]\n",
    "store_name = dataset_key + \".zarr\"\n",
    "\n",
    "ds = dsets[dataset_key]\n",
    "ds\n",
    "\n",
    "# Note that the xarray dataset summary includes a 'member_id' coordinate, which is a renaming of the \n",
    "# 'na-cordex-models' column in the catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Subsetting and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to Create a Single Map Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMap(ax, map_slice, date_object=None, member_id=None):\n",
    "    \"\"\"Create a map plot on the given axes, with min/max as text\"\"\"\n",
    "\n",
    "    ax.imshow(map_slice, origin='lower')\n",
    "\n",
    "    minval = map_slice.min(dim = ['lat', 'lon'])\n",
    "    maxval = map_slice.max(dim = ['lat', 'lon'])\n",
    "\n",
    "    # Format values to have at least 4 digits of precision.\n",
    "    ax.text(0.01, 0.03, \"Min: %3g\" % minval, transform=ax.transAxes, fontsize=12)\n",
    "    ax.text(0.99, 0.03, \"Max: %3g\" % maxval, transform=ax.transAxes, fontsize=12, horizontalalignment='right')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    if date_object:\n",
    "        ax.set_title(date_object.values.astype(str)[:10], fontsize=12)\n",
    "        \n",
    "    if member_id:\n",
    "        ax.set_ylabel(member_id, fontsize=12)\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function for Finding Dates with Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidDateIndexes(member_slice):\n",
    "    \"\"\"Search for the first and last dates with finite values.\"\"\"\n",
    "    min_values = member_slice.min(dim = ['lat', 'lon'])\n",
    "    is_finite = np.isfinite(min_values)\n",
    "    finite_indexes = np.where(is_finite)\n",
    "\n",
    "    start_index = finite_indexes[0][0]\n",
    "    end_index = finite_indexes[0][-1]\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function for Truncating Data Slices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncateData(data_slice, num_years):\n",
    "    \"\"\"Remove all but the first num_years of valid data from the data slice.\"\"\"\n",
    "    start_index, _ = getValidDateIndexes(data_slice)\n",
    "    start_date = data_slice.time[start_index]\n",
    "    start_year = start_date.dt.year.values\n",
    "    year_range = start_year + np.arange(num_years)\n",
    "    \n",
    "    data_slice = data_slice.isel(time=ds.time.dt.year.isin(year_range))\n",
    "    return data_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Producing Maps of First, Middle, and Final Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_mid_last(ds, data_var, store_name):\n",
    "    \"\"\"Plot the first, middle, and final time steps for several climate runs.\"\"\"\n",
    "    num_members_to_plot = 4\n",
    "    member_names = ds.coords['member_id'].values[0:num_members_to_plot]\n",
    "    \n",
    "    figWidth = 18 \n",
    "    figHeight = 12 \n",
    "    numPlotColumns = 3\n",
    "    fig, axs = plt.subplots(num_members_to_plot, numPlotColumns, figsize=(figWidth, figHeight), constrained_layout=True)\n",
    "\n",
    "    for index in np.arange(num_members_to_plot):\n",
    "        mem_id = member_names[index]\n",
    "        data_slice = ds[data_var].sel(member_id=mem_id)\n",
    "           \n",
    "        start_index, end_index = getValidDateIndexes(data_slice)\n",
    "        midDateIndex = np.floor(len(ds.time) / 2).astype(int)\n",
    "\n",
    "        startDate = ds.time[start_index]\n",
    "        first_step = data_slice.sel(time=startDate) \n",
    "        ax = axs[index, 0]\n",
    "        plotMap(ax, first_step, startDate, mem_id)\n",
    "\n",
    "        midDate = ds.time[midDateIndex]\n",
    "        mid_step = data_slice.sel(time=midDate)   \n",
    "        ax = axs[index, 1]\n",
    "        plotMap(ax, mid_step, midDate)\n",
    "\n",
    "        endDate = ds.time[end_index]\n",
    "        last_step = data_slice.sel(time=endDate)            \n",
    "        ax = axs[index, 2]\n",
    "        plotMap(ax, last_step, endDate)\n",
    "        \n",
    "        plt.suptitle(f'First, Middle, and Last Timesteps for Selected Runs in \"{store_name}\"', fontsize=20)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Producing Statistical Map Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat_maps(ds, data_var, store_name, truncate_data):\n",
    "    \"\"\"Plot the mean, min, max, and standard deviation values for several climate runs, aggregated over time.\"\"\"\n",
    "    \n",
    "    num_members_to_plot = 4\n",
    "    member_names = ds.coords['member_id'].values[0:num_members_to_plot]\n",
    "\n",
    "    figWidth = 25 \n",
    "    figHeight = 12 \n",
    "    numPlotColumns = 4\n",
    "    \n",
    "    fig, axs = plt.subplots(num_members_to_plot, numPlotColumns, figsize=(figWidth, figHeight), constrained_layout=True)\n",
    "\n",
    "    for index in np.arange(num_members_to_plot):\n",
    "        mem_id = member_names[index]\n",
    "        data_slice = ds[data_var].sel(member_id=mem_id)\n",
    "        \n",
    "        if truncate_data:\n",
    "            # Limit time range to three years.\n",
    "            data_slice = truncateData(data_slice, 3)\n",
    "\n",
    "        # Save slice in memory to prevent repeated disk loads\n",
    "        data_slice = data_slice.persist()\n",
    "\n",
    "        data_agg = data_slice.min(dim='time')\n",
    "        plotMap(axs[index, 0], data_agg, member_id=mem_id)\n",
    "\n",
    "        data_agg = data_slice.max(dim='time')\n",
    "        plotMap(axs[index, 1], data_agg)\n",
    "\n",
    "        data_agg = data_slice.mean(dim='time')\n",
    "        plotMap(axs[index, 2], data_agg)\n",
    "\n",
    "        data_agg = data_slice.std(dim='time')\n",
    "        plotMap(axs[index, 3], data_agg)\n",
    "\n",
    "    axs[0, 0].set_title(f'min({data_var})', fontsize=15)\n",
    "    axs[0, 1].set_title(f'max({data_var})', fontsize=15)\n",
    "    axs[0, 2].set_title(f'mean({data_var})', fontsize=15)\n",
    "    axs[0, 3].set_title(f'std({data_var})', fontsize=15)\n",
    "\n",
    "    plt.suptitle(f'Spatial Statistics for Selected Runs in \"{store_name}\"', fontsize=20)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Producing Time Series Plots\n",
    "Also show which dates have no available data values, as a rug plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(ds, data_var, store_name, truncate_data):\n",
    "    \"\"\"Plot the mean, min, max, and standard deviation values for several climate runs, \n",
    "       aggregated over lat/lon dimensions.\"\"\"\n",
    "\n",
    "    num_members_to_plot = 4\n",
    "    member_names = ds.coords['member_id'].values[0:num_members_to_plot]\n",
    "\n",
    "    figWidth = 25 \n",
    "    figHeight = 20\n",
    "    linewidth = 0.5\n",
    "\n",
    "    numPlotColumns = 1\n",
    "    fig, axs = plt.subplots(num_members_to_plot, numPlotColumns, figsize=(figWidth, figHeight))\n",
    "        \n",
    "    for index in np.arange(num_members_to_plot):\n",
    "        mem_id = member_names[index]\n",
    "        data_slice = ds[data_var].sel(member_id=mem_id)\n",
    "        \n",
    "        if truncate_data:\n",
    "            # Limit time range to three years.\n",
    "            data_slice = truncateData(data_slice, 3)\n",
    "\n",
    "        # Save slice in memory to prevent repeated disk loads\n",
    "        data_slice = data_slice.persist()\n",
    "\n",
    "        unit_string = ds[data_var].attrs['units']\n",
    "        \n",
    "        min_vals = data_slice.min(dim = ['lat', 'lon'])\n",
    "        max_vals = data_slice.max(dim = ['lat', 'lon'])\n",
    "        mean_vals = data_slice.mean(dim = ['lat', 'lon'])\n",
    "        std_vals = data_slice.std(dim = ['lat', 'lon'])\n",
    "\n",
    "        missing_indexes = np.isnan(min_vals).compute()\n",
    "        missing_times = data_slice.time[missing_indexes]\n",
    "\n",
    "            \n",
    "        axs[index].plot(data_slice.time, max_vals, linewidth=linewidth, label='max', color='red')\n",
    "        axs[index].plot(data_slice.time, mean_vals, linewidth=linewidth, label='mean', color='black')\n",
    "        axs[index].fill_between(data_slice.time, (mean_vals - std_vals), (mean_vals + std_vals), \n",
    "                                         color='grey', linewidth=0, label='std', alpha=0.5)\n",
    "        axs[index].plot(data_slice.time, min_vals, linewidth=linewidth, label='min', color='blue')\n",
    "            \n",
    "        # Produce a rug plot along the bottom of the figure for missing data.\n",
    "        ymin, ymax = axs[index].get_ylim()\n",
    "        rug_y = ymin + 0.01*(ymax-ymin)\n",
    "        axs[index].plot(missing_times, [rug_y]*len(missing_times), '|', color='m', label='missing')\n",
    "        axs[index].set_title(mem_id, fontsize=20)\n",
    "        axs[index].legend(loc='upper right')\n",
    "        axs[index].set_ylabel(unit_string)\n",
    "\n",
    "    plt.tight_layout(pad=10.2, w_pad=3.5, h_pad=3.5)\n",
    "    plt.suptitle(f'Temporal Statistics for Selected Runs in \"{store_name}\"', fontsize=20)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Diagnostic Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot First, Middle, and Final Timesteps for Several Output Runs (less compute intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Plot using the Zarr Store obtained from an earlier step in the notebook.\n",
    "figure = plot_first_mid_last(ds, data_var, store_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save figure to a PNG file\n",
    "\n",
    "Change the value of `SAVE_PLOT` to True to produce a PNG file of the plot.   The file will be saved in the same folder as this notebook.\n",
    "\n",
    "Then use Jupyter's file browser to locate the file and right-click the file to download it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PLOT = False\n",
    "if SAVE_PLOT:\n",
    "    plotfile = f'./{dataset_key}_FML.png'\n",
    "    figure.savefig(plotfile, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Statistical Map Plots for Several Output Runs (more compute intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Plot using the Zarr Store obtained from an earlier step in the notebook.\n",
    "figure = plot_stat_maps(ds, data_var, store_name, TRUNCATE_DATA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save figure to a PNG file\n",
    "\n",
    "Change the value of `SAVE_PLOT` to True to produce a PNG file of the plot.   The file will be saved in the same folder as this notebook.\n",
    "\n",
    "Then use Jupyter's file browser to locate the file and right-click the file to download it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PLOT = False\n",
    "if SAVE_PLOT:\n",
    "    plotfile = f'./{dataset_key}_MAPS.png'\n",
    "    figure.savefig(plotfile, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Time Series for Several Output Runs (more compute intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Plot using the Zarr Store obtained from an earlier step in the notebook.\n",
    "figure = plot_timeseries(ds, data_var, store_name, TRUNCATE_DATA)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save figure to a PNG file\n",
    "\n",
    "Change the value of `SAVE_PLOT` to True to produce a PNG file of the plot.   The file will be saved in the same folder as this notebook.\n",
    "\n",
    "Then use Jupyter's file browser to locate the file and right-click the file to download it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PLOT = False\n",
    "if SAVE_PLOT:\n",
    "    plotfile = f'./{dataset_key}_TS.png'\n",
    "    figure.savefig(plotfile, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Show which python package versions were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
